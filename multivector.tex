%}}
% Copyright © 2020 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%{
\input{../latex/blogpost.tex}
\renewcommand{\basename}{multivector}
%\renewcommand{\dirname}{notes/phy1520/}
\renewcommand{\dirname}{notes/ece1228-electromagnetic-theory/}
%\newcommand{\dateintitle}{}
%\newcommand{\keywords}{}

\input{../latex/peeter_prologue_print2.tex}

\usepackage{peeters_braket}
\usepackage{peeters_figures}
\usepackage{siunitx}
\usepackage{verbatim}
\usepackage{peeters_tablebox}
\usepackage{peeters_layout_exercise}
\usepackage{macros_qed}
%\usepackage{mhchem} % \ce{}
%\usepackage{macros_bm} % \bcM
%\usepackage{macros_qed} % \qedmarker
%\usepackage{txfonts} % \ointclockwise

\newcommand{\nbcite}[2]{%
#2%
%\itemCite{GAelectrodynamics}{#1}{#2}%
}

% \mathImageFigure{path}{caption}{label}{width}{nbpath}
% nbpath like: ps2b:countItersAndPlot.m
\newcommand{\mathImageFigure}[5]{%
\imageFigure{#1}{\nbcite{#5}{#2}}{#3}{#4}
}

\beginArtNoToc

\generatetitle{An new axiomatic introduction of multivectors, vector products, and geometric algebra.}
%\chapter{Multivector}
%\label{chap:multivector}

\paragraph{Motivation.}
Many introductions to geometric algebra start by first introducing the dot product, then bivectors and the wedge product, and eventually define the product of two vectors as the synthetic sum of the dot and wedge
\begin{equation}\label{eqn:multivector:20}
\Bx \By = \Bx \cdot \By + \Bx \wedge \By.
\end{equation}
It takes a fair amount of work to do this well.  In the seminal work \citep{hestenes1999nfc} a few pages are taken for each of the dot and wedge products, showing the similarities and building up ideas, before introducing the geometric product in this fashion.  In \citep{dorst2007gac} the authors take a phenomenal five chapters to build up the context required to introduce the geometric product.
I am not disparaging the authors for taking that long to build up the ideas, as their introduction of the subject is exceedingly clear and thorough, and they do a lot more than the minumum required to define the geometric product.

The strategy to introduce the geometric product as a sum of dot and wedge can result in considerable confusion, especially since the wedge product is often defined in terms of the geometric product
\begin{equation}\label{eqn:multivector:40}
\Bx \wedge \By =
\inv{2} \lr{
\Bx \By - \By \Bx
}.
\end{equation}
The whole subject can appear like a chicken and egg problem.  I personally found the subject very confusing initially, and had considerable difficulty understanding which of the many
identities of geometric algebra were the most fundamental.  For this reason, I found the axiomatic approach of \citep{doran2003gap} very refreshing.  The cavaet with that work is that is is exceptionally terse, as they jammed a reformulation of most of physics using geometric algebra into that single book, and it would have been thousands of pages had they tried to make it readable by mere mortals.

When I wrote my own book on the subject, I had the intuition that the way to introduce the subject ought to be like the vector space in abstract linear algebra.  The construct of a vector space is a curious and indirect way to define a vector.  Vectors are not defined as entities, but simply as members of a vector space, a space that is required to have a set of properties.  I thought that the same approach would probably work with multivectors, which could be defined as members of a multivector space, a mathematical construction with a set of properties.

I did try this approach, but was not fully satisfied with what I wrote.  I think that dissatisfaction was because I tried to define the multivector first.  To define the multivector, I first introduced a whole set of
prerequisite ideas (bivector, trivector, blade, k-vector, vector product, ...), but that was also problematic, since the vector multiplication idea required for those concepts wasn't fully defined until the multivector space itself was defined.

My approach shows some mathematical cowardness.  Had I taken the approach of the vector space fully to heart, the multivector could have been defined as a member of a multivector space, and all the other ideas follow from that.  In this article, I'm going to play with this approach anew, and see how it works out.
\paragraph{Vectors.}
In engineering vectors are often represented as column matrixes.  A vector with coordinates \((x,y,z)\) would have the representation
\begin{dmath}\label{eqn:multivector:60}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix},
\end{dmath}
where the unit vectors in those respective directions are
\begin{equation}\label{eqn:prerequisites:20}
\Be_1 =
\begin{bmatrix}
1 \\
0 \\
0 \\
\end{bmatrix},\quad
\Be_2 =
\begin{bmatrix}
0 \\
1 \\
0 \\
\end{bmatrix},\quad
\Be_3 =
\begin{bmatrix}
0 \\
0 \\
1 \\
\end{bmatrix}.
\end{equation}
I'll use the symbol \( \Be_i \) to designate the unit vector in the ith direction%
\footnote{
The notation for the unit vectors themselves varies by author.
For example, it's not uncommon in engineering texts to use \( \hat{a}_x, \hat{a}_y, \hat{a}_z \) instead of \( \Be_1, \Be_2, \Be_3 \).  I would guess that the \(\hat{a}\) notation evolved to avoid the overloading of the symbol \(e = 2.718\cdots\).}.%
This symbolic designation allows any vector to be encoded in a
representation agnostic fashion.  For example a vector \( \Bx \) with coordinates \( x, y, z \) is
\begin{dmath}\label{eqn:prerequisites:40}
\Bx = x \Be_1 + y \Be_2 + z \Be_3,
\end{dmath}
independent of whether the underlying representation of the unit vectors themselves are tuple, row, column, or anything else.

With a coordinate representation, we can add and subtract vectors by simply adding and subtracting their coordinates.  This corresponds to the graphical operation of chaining vectors connected head to tail and drawing the resulting vector that connects the initial tail to the final head, as illustrated in
\cref{fig:vectorAddition:vectorAdditionFig1}.
\mathImageFigure{../figures/GAelectrodynamics/vectorAdditionFig1}{Addition of vectors.}{fig:vectorAddition:vectorAdditionFig1}{0.3}{vectorOrientationAndAdditionFigures.nb}

We must supplement a coordinate vector representation with an operation that provides the length of the vector.  In engineering this is a matrix operation
\begin{dmath}\label{eqn:multivector:80}
\Norm{\Bx} = \sqrt{ \Bx^\T \Bx },
\end{dmath}
which is a compact encoding of Euclidean length \( \sqrt{x^2 + y^2 + z^2} \).  With addition, subtraction, and length operations defined for a set of coordinates, we now have a justification for calling those coordinates a representation of a vector, ``a quantity with magnitude and direction''.

It is useful to be able to generalize the length operation above by defining a dot product
\begin{dmath}\label{eqn:multivector:100}
\Bx \cdot \By = \Bx^\T G \By,
\end{dmath}
where \( G \) is an \( N \times N \) symmetric matrix, and \( \Bx, \By \) are \( N \times 1 \) column vectors.  The matrix \( G \), also called the metric, is often the identity matrix, in which case the dot product of a vector with itself is the squared (Euclidean) length of that vector.  A common metric in physics is that of special relativistic spacetime
\begin{dmath}\label{eqn:multivector:120}
G =
\pm
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 0 & 0 & -1 \\
\end{bmatrix},
\end{dmath}
where the squared-length of ``timelike'' or ``spacelike'' vectors have a different sign.  Observe that this special relativistic dot product does not have the usual \( \Bx \cdot \Bx \ge 0 \) property that is often included in an axiomatic definition of a dot product.

%We can, for example, represent vectors in two, three, or \(N \) dimensions as the coordinates of the arrow heads.
%\makedefinition{\R{N}}{definition:prerequisites:RN}{
%Define \R{N} as the set of tuples \( \setlr{ (x_1, x_2, \cdots) \mid x_i \in \bbR } \).
%}
%By itself, a set of tuples, need not have any addition, nor multiplication operation.  As soon as we define such operations (in the obvious fashion), it is straightforward (even to the point of tedium) that the construction satisfies
%the requirements of a vector space.
%Representing a vector as a set of coordinates cannot represent the idea that a vector is a quantity with direction and magnitude unless it is also supplemented with a length operation.  In engineering that length operation is is also a matrix
% is a useful abstraction.  When coupled with an operation that  and encodes the more simplistic notion that a vector is a quantity with
%, but not terribly geometric.
%\  , it is consistent with the ``directed arrow'' concept of vectors that we first learn.  For example, if we add two vectors, as illustrated in
%, we end up with another vector (provided the vector space contains all vectors in the plane of the two summands.)
%If we scale a vector, as illustrated in \cref{fig:VectorsWithOppositeOrientation:VectorsWithOppositeOrientationFig1}, we also end up with another vector (provided the vector space contains all vectors that lie in the direction of the original.)  The reader should convince themselves that the graphical concept of vectors (a directed arrow) satisfies all the properties of a vector in a vector space.
%\mathImageFigure{../figures/GAelectrodynamics/VectorsWithOppositeOrientationFig1}{Scalar multiples of vectors.}{fig:VectorsWithOppositeOrientation:VectorsWithOppositeOrientationFig1}{0.15}{vectorOrientationAndAdditionFigures.nb}
%
%In engineering, we usually represent vectors by column matrices, whereas in geometric algebra, the representation (tuple, row-vector, column-vector, ...) is typically irrelevant.  We can bridge the two paradigms by introducing standard basis elements.
%For example, in three dimensional space with a column vector representation, the respective unit vectors along each of the \(x\), \(y\), and \(z\) directions are
\paragraph{Vector space.}
Since the inspiration of everything that follows is the definition of a vector space, we should remind ourselves how that was defined.
\makedefinition{Vector space.}{def:prerequisites:vectorspace}{
A vector space is a set \( V = \setlr{\Bx, \By, \Bz, \cdots} \), the elements of which are called vectors, which has an addition operation designated \( + \) and a scalar multiplication operation designated by juxtaposition, where the following axioms are satisfied
for all vectors \( \Bx, \By, \Bz \in V \) and scalars \( a, b \in \bbR \).
\begin{tablebox}[tabularx={X|Y}]%{Vector space axioms.}
    V is closed under addition & \( \Bx + \By \in V \) \\ \hline
    V is closed under scalar multiplication & \( a \Bx \in V \) \\ \hline
    Addition is associative & \( (\Bx + \By) + \Bz = \Bx + (\By + \Bz) \) \\ \hline
    Addition is commutative & \( \By + \Bx = \Bx + \By \) \\ \hline
    There exists a zero element \( \Bzero \in V \)  & \( \Bx + \Bzero = \Bx \) \\ \hline
    For any \( \Bx \in V \) there exists a negative additive inverse \( -\Bx \in V \) & \( \Bx + (-\Bx) = \Bzero \) \\ \hline
    Scalar multiplication is distributive  & \( a( \Bx + \By ) = a \Bx + a \By \), \( (a + b)\Bx = a \Bx + b\Bx \) \\ \hline
    Scalar multiplication is associative & \( (a b) \Bx = a ( b \Bx ) \) \\ \hline
    There exists a multiplicative identity & \( 1 \Bx = \Bx \) \\ \hline
\end{tablebox}
} % makedefinition{Vector space.}
%A vector space is a set \( V = \setlr{\Bx, \By, \Bz, \cdots} \), the elements of which are called vectors, which has an addition operation designated \( + \) and a scalar multiplication operation designated by juxtaposition, where the following axioms are satisfied
%for all vectors \( \Bx, \By, \Bz \in V \) and scalars \( a, b \in \bbR \).
%
%\begin{tabular}{|l|l|}
%\hline
%V is closed under addition & \( \Bx + \By \in V \)  \\ \hline
%V is closed under scalar multiplication & \( a \Bx \in V \)  \\ \hline
%Addition is associative & \( (\Bx + \By) + \Bz = \Bx + (\By + \Bz) \)  \\ \hline
%Addition is commutative & \( \By + \Bx = \Bx + \By \)  \\ \hline
%There exists a zero element \( \Bzero \in V \)  & \( \Bx + \Bzero = \Bx \)  \\ \hline
%For any \( \Bx \in V \) there exists a negative additive inverse \( -\Bx \in V \) & \( \Bx + (-\Bx) = \Bzero \)  \\ \hline
%Scalar multiplication is distributive  & \( a( \Bx + \By ) = a \Bx + a \By \), \( (a + b)\Bx = a \Bx + b\Bx \)  \\ \hline
%Scalar multiplication is associative & \( (a b) \Bx = a ( b \Bx ) \)  \\ \hline
%There exists a multiplicative identity & \( 1 \Bx = \Bx \) \\ \hline
%\end{tabular}
While this concept of vector space is an abstract beast
FIXME: SNIP.
\paragraph{SNIP.}
Let's show this.
\makeproblem{\R{N}}{problem:prerequisites:RN}{
Show that \R{N} is a vector space when the
addition operation is defined as
\( \Bx + \By \equiv (x_1 + y_1, x_2 + y_2, \cdots) \)
, and
scalar multiplication
is defined as
\( a \Bx \equiv (a x_1 , a x_2 , \cdots ) \) for any
\( \Bx = (x_1, x_2, \cdots) \in \bbR^N \),
\( \By = (y_1, y_2, \cdots) \in \bbR^N \), and
\( a \in \bbR \).
} % problem
\makeanswer{problem:prerequisites:RN}{
\begin{itemize}
\item Closed with respect to addition:
Given \( \Bx, \By \) defined above, we have
\( \Bx + \By = (x_1 + y_1, x_2 + y_2, \cdots) \in \bbR^N \).
\item Closed with respect to multiplication:
\( a \Bx = (a x_1, a x_2, \cdots) \in \bbR^N \).
\item Addition is associative, commutative: left to the reader to verify.
\item Zero element:
Given \( \Bzero = (0, 0, \cdots)\), clearly \( \Bx + \Bzero = (x_1 + 0, x_2 + 0, \cdots) = \Bx \) for any \( \Bx \in \bbR^N \).
\item Negative inverse.
Given \( -\Bx = (-x_1, -x_2, \cdots)\), then \( \Bx + (-\Bx) = (x_1 - x_1, x_2 - x_2, \cdots) = \Bzero\).  Clearly we can construct a negative additive inverse for any \( \Bx \).
\item Distributed and associative nature of scalar multiplication : left to the reader to verify.
\item Multiplicative identity:
\( 1 \Bx = 1 (x_1, x_2, \cdots) = (1 x_1, 1 x_2, \cdots) = \Bx. \qedmarker\)
\end{itemize}
} % answer
For geometric algebra, we do not actually need any abstract vector spaces, but two additional examples are provided here as problems for the interested reader.
\input{../GAelectrodynamics/functionvectorspace.tex}
\input{../GAelectrodynamics/paulivectorspace.tex}
\paragraph{REWRITE FROM HERE.}

%In geometric algebra, we require what can loosely be called a dot product.
%A dot product usually has the following characteristics
%
%- Symmetric : $ \Bx \cdot \By = \By \cdot \Bx $
%- Bilinear : $ (a \Bx + b \By) \cdot \Bz = a \Bx \cdot \Bz + b \By \cdot \Bz,\quad \Bx \cdot (a \By + b \Bz) = a \Bx \cdot \By + b \Bx \cdot \Bz $
%- Positive length : $ \Bx \cdot \Bx > 0, \Bx \ne 0 $
%
%, but the positive definite nature of that dot product is not required.
%
If a vector space \( V \) contains elements \( \Bx, \By \), we designate that dot product as \( \Bx \cdot \By \), and require

Symmetric : \( \Bx \cdot \By = \By \cdot \Bx \)

Bilinear : \( (a \Bx + b \By) \cdot \Bz = a \Bx \cdot \Bz + b \By \cdot \Bz,\quad \Bx \cdot (a \By + b \Bz) = a \Bx \cdot \By + b \Bx \cdot \Bz \)

Positive length : \( \Bx \cdot \Bx > 0, \Bx \ne 0 \)


Recall that a vector space with an associated dot product is called a dot product space.

Given a finite dimensional (dot-product) vector space \( V = \setlr{ \Bx, \By, \Bz, \cdots } \), with a dot product where the dot product of elements
, with a dot product \( \Bx \cdot \By \)
a multivector space generated by \( V \) is a set \( M = \setlr{ x, y, z, \cdots } \) of multivectors (sums of scalars, vectors, or products of vectors), where the following axioms are satisfied

Contraction : \( \Bx^2 = \Bx \cdot \Bx, \,\forall \Bx \in V \)

\( M \) is closed under addition : \( x + y \in M \)

\( M \) is closed under multiplication : \( x y \in M \)

Addition is associative : \( (x + y) + z = x + (y + z) \)

Addition is commutative : \( y + x = x + y \)

There exists a zero element \( 0 \in M \)  : \( x + 0 = x \)

For all \( x \in M \) there exists a negative additive inverse \( -x \in M \) : \( x + (-x) = 0 \)

Multiplication is distributive  : \( x( y + z ) = x y + x z \), \( (x + y)z = x z + y z \)

Multiplication is associative : \( (x y) z = x ( y z ) \)

There exists a multiplicative identity \( 1 \in M \) : \( 1 x = x \)

Clearly $\mathbb{R}$, using scalar multiplication as the dot product, is a multivector space.

It's possible to show that $\mathbb{R}^2$, $\mathbb{R}^3$, and other vector spaces (with the normal Euclidean dot product) also generate multivector spaces.  Both of these first require that we show that \( \Bx \By = - \By \Bx \), if \( \Bx \cdot \By = 0 \), that is, the products of perpendicular vectors, assumed to be members of  anticommute.

%}
\EndArticle
