%
% Copyright © 2024 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%{
\input{../latex/blogpost.tex}
\renewcommand{\basename}{vectorBivector}
%\renewcommand{\dirname}{notes/phy1520/}
\renewcommand{\dirname}{notes/ece1228-electromagnetic-theory/}
%\newcommand{\dateintitle}{}
%\newcommand{\keywords}{}

\input{../latex/peeter_prologue_print2.tex}

\usepackage{peeters_layout_exercise}
\usepackage{peeters_braket}
\usepackage{peeters_figures}
\usepackage{siunitx}
\usepackage{verbatim}
\usepackage{amsthm} % proof
%\usepackage{mhchem} % \ce{}
%\usepackage{macros_bm} % \bcM
%\usepackage{macros_qed} % \qedmarker
%\usepackage{txfonts} % \ointclockwise

\beginArtNoToc

\generatetitle{Vector-vector and vector-bivector products in geometric algebra}
%\chapter{Vector-vector and vector-bivector products in geometric algebra}
%\label{chap:vectorBivector}

\section{Motivation.}
I regularly see examples where people are confused about the identities of geometric algebra.  Some texts on the subject present a barrage of identites, and one is left confused about which ones are fundamental, which ones are derived.  People also regularily get confused about identities that apply to vectors, but may not apply to bivectors or multivectors.  I will systemcally examine some of these basic relationships here from first principles.  I will use coordinate representations of vectors here, despite the fact that doing so is considered very rude by some geometric algebra practioners.

\section{Vector products.}
\subsection{Prerequisites.}
Let's look at vector-vector products and bivector-vector products without using any identities, and see what we come up with.  We will use the contraction axiom
\makedefinition{Contraction axiom.}{dfn:vectorBivector:1}{
Given vector \( \Bx \),
\begin{equation*}
\Bx^2 = \Bx \cdot \Bx.
\end{equation*}
} % definition
There are a couple consequences of this that are easy to show.
\maketheorem{Basis element products.}{thm:vectorBivector:1}{
Given a standard basis \( \setlr{ \Be_i } \), where \( \Be_i \cdot \Be_j = \delta_{ij} \),
\begin{equation*}
\Be_i \Be_i = 1,
\end{equation*}
\begin{equation*}
\Be_i \Be_j = -\Be_j \Be_i, \quad i \ne j.
\end{equation*}
} % theorem
\begin{proof}
The first obviously follows directly from the contraction axiom.  For the second we may expand the square of \( \Be_i + \Be_j \), where \( i \ne j \), we find that
\begin{equation}\label{eqn:vectorBivector:21}
\begin{aligned}
\lr{ \Be_i + \Be_j }^2 &= \Be_i^2 + \Be_j^2 + \Be_i \Be_j + \Be_j + \Be_i \\
&= 2 + \Be_i \Be_j + \Be_j + \Be_i
\end{aligned}
\end{equation}
but using the contraction axiom, we also have
\begin{equation}\label{eqn:vectorBivector:41}
\begin{aligned}
\lr{ \Be_i + \Be_j }^2 &= \lr{ \Be_i + \Be_j } \cdot \lr{ \Be_i + \Be_j } \\
&= 2,
\end{aligned}
\end{equation}
so, comparison shows
\begin{equation}\label{eqn:vectorBivector:61}
\Be_i \Be_j + \Be_j \Be_i = 0.
\end{equation}
\end{proof}
\subsection{Products.}
With the basics established, we have what we need to understand products of vectors.  Let's start first with a couple examples.

Let \( \By = b \Bx \), where \( b \) is a constant, and \( \Bx \cdot \Bx = a \).  The product \( \Bx \By \) is
\begin{equation}\label{eqn:vectorBivector:81}
\begin{aligned}
\Bx \By
&= \Bx b \Bx \\
&= b \Bx^2 \\
&= b \Bx \cdot \Bx \\
&= b a.
\end{aligned}
\end{equation}
We see that the product of any two colinear vectors is a scalar.

Let \( \Bx = \Be_3, \By = \Be_1 + \Be_2 \).  The product \( \Bx \By \) is
\begin{equation}\label{eqn:vectorBivector:101}
\begin{aligned}
\Bx \By
&= \Be_3 \lr{ \Be_1 + \Be_2 } \\
&= \Be_3 \Be_1 + \Be_3 \Be_2.
\end{aligned}
\end{equation}
We see in this case that the product of these perpendicular vectors is a bivector.  It is also generally true that products of perpendicular vectors are bivectors.

Let \( \Bx = \Be_1, \By = \Be_1 + \Be_2 \).  The product \( \Bx \By \) is
\begin{equation}\label{eqn:vectorBivector:121}
\begin{aligned}
\Bx \By
&= \Be_1 \lr{ \Be_1 + \Be_2 } \\
&= \Be_1^2 + \Be_1 \Be_2 \\
&= 1 + \Be_1 \Be_2.
\end{aligned}
\end{equation}
In this example, where the two vectors were neither colinear, nor perpendicular, their product had both scalar and bivector components.

The scalar part of a vector-vector product is actually related to the dot product.  Before showing that we need a bit of notation.
\makedefinition{Grade selection.}{dfn:vectorBivector:2}{
Given \( A_i \in \bigwedge^i \), and multivector \( A = \sum_i A_i \), the grade selection operation is designated with angle brackets
\begin{equation*}
A_i = \gpgrade{A}{i}.
\end{equation*}
Grade-0 selection is given the special shorthand
\begin{equation*}
\gpgradezero{A} = \gpgrade{A}{0}.
\end{equation*}
} % definition

As an example, suppose that \( A = 1 + 2 \Be_1 + 3 \Be_3 + 4 \Be_1 \Be_3 + 5 \Be_1 \Be_2 \Be_3 \), for which we have
\begin{equation}\label{eqn:vectorBivector:821}
\begin{aligned}
\gpgradezero{A} &= 1 \\
\gpgradeone{A} &= 2 \Be_1 + 3 \Be_3 \\
\gpgradetwo{A} &= 4 \Be_1 \Be_3 \\
\gpgradethree{A} &= 5 \Be_1 \Be_2 \Be_3.
\end{aligned}
\end{equation}

Given a grade-k, vector, application of the grade selection operator for any other grade is zero.  For example, given \( A = \Be_1 \Be_2, B = \Be_1 \Be_2 \Be_3 \), we have
\begin{equation}\label{eqn:vectorBivector:841}
\begin{aligned}
0 &= \gpgradezero{ A } = \gpgradeone{ A } = \gpgradethree{A} \\
0 &= \gpgradezero{ B } = \gpgradeone{ B } = \gpgradetwo{A} \\
\end{aligned}
\end{equation}

We may use this grade selection notation to express the dot product of two vectors.
\maketheorem{Dot product of vectors.}{thm:vectorBivector:5}{
Given two vectors \( \Bx, \By \), their dot product is
\begin{equation*}
\Bx \cdot \By = \gpgradezero{\Bx \By}.
\end{equation*}
} % theorem
\begin{proof}
Let \( \Bx = \sum_i x_i \Be_i, \By = \sum_j y_j \Be_j \), so that
\begin{equation}\label{eqn:vectorBivector:141}
\begin{aligned}
\gpgradezero{ \Bx \By }
&= \gpgradezero{ \sum_{ij} x_i y_j \Be_i \Be_j } \\
&= \gpgradezero{ \sum_{i = j} x_i y_j \Be_i \Be_j + \sum_{i \ne j} x_i y_j \Be_i \Be_j }.
\end{aligned}
\end{equation}
This second sum has only bivector grades, so it's grade-0 selection is empty.  That leaves us with
\begin{equation}\label{eqn:vectorBivector:161}
\begin{aligned}
\gpgradezero{ \Bx \By }
&= \sum_i \gpgradezero{ x_i y_i \Be_i \Be_i } \\
&= \sum_i x_i y_i \gpgradezero{ \Be_i^2 } \\
&= \sum_i x_i y_i.
\end{aligned}
\end{equation}
\end{proof}

Before considering the general case, we want a couple definitions, and one theorem
\makedefinition{Dot and wedge product of k-vectors.}{dfn:vectorBivector:3}{
If \( A_m \in \bigwedge^m, B_n \in \bigwedge^n \), then we define the dot and wedge products respectively as the following grade selections
\begin{equation*}
A_m \cdot B_n = \gpgrade{A_m B_n}{\Abs{m - n}}
\end{equation*}
\begin{equation*}
A_m \wedge B_n = \gpgrade{A_m B_n}{m + n}.
\end{equation*}
} % definition

In particular, for two vectors \( A_1 = \Bx, B_1 = \By \), this definition implies that
\begin{equation}\label{eqn:vectorBivector:181}
\begin{aligned}
\Bx \cdot \By = \gpgrade{ \Bx \By }{\Abs{1 -1}} = \gpgradezero{ \Bx \By },
\end{aligned}
\end{equation}
as we expect.

Armed with suitable definitions, we can now find one of the fundamental identities of geometric algebra.
\maketheorem{Product of vectors.}{thm:vectorBivector:7}{
Given two vectors \( \Bx, \By \),
\begin{equation*}
\Bx \By = \Bx \cdot \By + \Bx \wedge \By.
\end{equation*}
} % theorem
\begin{proof}
We saw above that the product of two vectors had the following coordinate expansion
\begin{equation}\label{eqn:vectorBivector:201}
\Bx \By = \sum_i x_i y_i + \sum_{i \ne j} x_i y_j \Be_i \Be_j.
\end{equation}
This clearly has only scalar and bivector grades, so we may write our product as a sum of grade selections
\begin{equation}\label{eqn:vectorBivector:221}
\Bx \By
= \gpgradezero{ \Bx \By} + \gpgradetwo{ \Bx \By }.
\end{equation}
From our definitions of the general k-vector dot and wedge products, this is
\begin{equation}\label{eqn:vectorBivector:241}
\Bx \By
= \Bx \cdot \By + \Bx \wedge \By.
\end{equation}
\end{proof}

The theorem above is too abstract at this point, as we have not discovered any of the properties of the wedge product.  Let's do that now.
\maketheorem{Properties of the wedge product of two vectors.}{thm:vectorBivector:261}{
Given vectors \( \Bw, \Bx, \By, \Bz \), and scalars \( a, b, c, d \)
\begin{equation*}
\Bx \wedge \By = \sum_{i < j}
\begin{vmatrix}
x_i & x_j \\
y_i & y_j \\
\end{vmatrix}
\Be_i \Be_j
\end{equation*}
\begin{equation*}
\Bx \wedge \By = - \By \wedge \Bx
\end{equation*}
\begin{equation*}
\lr{ a \Bw + b \Bx } \wedge \lr{ c \By + d \Bz } = a c \lr{ \Bw \wedge \By } + a d \lr{ \Bw \wedge \Bz } + b c \lr{ \Bx \wedge \By } + b d \lr{ \Bx \wedge \Bz }
\end{equation*}
\begin{equation*}
\Bx \wedge \Bx = 0.
\end{equation*}
} % theorem
\begin{proof}
We came close to the determinant expansion of the wedge product earlier, and just need a bit of index gymnastics to get the rest of the way there
\begin{equation}\label{eqn:vectorBivector:281}
\begin{aligned}
\Bx \wedge \By
&=
\sum_{i \ne j} x_i y_j \Be_i \Be_j \\
&=
\sum_{i < j} x_i y_j \Be_i \Be_j
+
\sum_{j < i} x_i y_j \Be_i \Be_j \\
&=
\sum_{i < j} x_i y_j \Be_i \Be_j
+
\sum_{i < j} x_j y_i \Be_j \Be_i \\
&=
\sum_{i < j} \lr{ x_i y_j - x_j y_i } \Be_i \Be_j \\
&=
\sum_{i < j} \begin{vmatrix}
x_i & x_j \\
y_i & y_j \\
\end{vmatrix}
\Be_i \Be_j.
\end{aligned}
\end{equation}

From this, we see immediately that swapping \( \Bx, \By \) gives
\begin{equation}\label{eqn:vectorBivector:301}
\begin{aligned}
\By \wedge \Bx &=
\sum_{i < j} \begin{vmatrix}
y_i & y_j \\
x_i & x_j \\
\end{vmatrix}
\Be_i \Be_j \\
&=
-\sum_{i < j} \begin{vmatrix}
x_i & x_j \\
y_i & y_j \\
\end{vmatrix}
\Be_i \Be_j \\
&= - \Bx \wedge \By,
\end{aligned}
\end{equation}
which proves the second property.

Proof of the bilinearity property is left to the reader.

When the vectors are equal, we have
\begin{equation}\label{eqn:vectorBivector:321}
\begin{aligned}
\Bx \wedge \Bx = - \Bx \wedge \Bx,.
\end{aligned}
\end{equation}
so \( \Bx \wedge \Bx = 0 \).
\end{proof}

\subsection{Wedge product filtering, and linear solutions.}
An important application of the wedge product is its filtering property.
\maketheorem{Wedge product filtering property.}{thm:vectorBivector:99}{
Given vectors \( \Bx, \By \)
\begin{equation*}
\Bx \wedge \lr{ a \Bx + \By } = \Bx \wedge \By.
\end{equation*}
} % theorem
\begin{proof}
This follows first by application of the linearity property
\begin{equation}\label{eqn:vectorBivector:341}
\begin{aligned}
\Bx \wedge \lr{ a \Bx + \By }
&= a \lr{ \Bx \wedge \Bx } + \Bx \wedge \By \\
&= \Bx \wedge \By,
\end{aligned}
\end{equation}
since \( \Bx \wedge \Bx = 0 \).
\end{proof}
Wedging a vector with a second, kills any components in the second that are colinear with the first.

This has many applications.  For example, we may solve a simple linear system
\begin{equation}\label{eqn:vectorBivector:361}
\begin{aligned}
a \Bx + b \By &= \Ba \\
c \Bx + d \By &= \Bb
\end{aligned}
\end{equation}
by wedging with one of the \( \Bx, \By \) vectors, eliminating it.  For example, wedging both equations with \( \Bx \) from the left, we have
\begin{equation}\label{eqn:vectorBivector:381}
\begin{aligned}
a \Bx \wedge \Bx + b \Bx \wedge \By &= \Bx \wedge \Ba \\
c \Bx \wedge \Bx + d \Bx \wedge \By &= \Bx \wedge \Bb
\end{aligned},
\end{equation}
or
\begin{equation}\label{eqn:vectorBivector:401}
\begin{aligned}
b \Bx \wedge \By &= \Bx \wedge \Ba \\
d \Bx \wedge \By &= \Bx \wedge \Bb
\end{aligned},
\end{equation}
when the system of equations has a solution, then it must be that all these wedge products are scalar multiples of each other.  That is, \( \Bx \wedge \By \propto \Bx \wedge \Ba \propto \Bx \wedge \Bb \), and we find
\begin{equation}\label{eqn:vectorBivector:421}
\begin{aligned}
b &= \frac{ \Bx \wedge \Ba }{\Bx \wedge \By } \\
d &= \frac{ \Bx \wedge \Bb }{\Bx \wedge \By }.
\end{aligned}
\end{equation}
For a two dimensional problem, this ratio is easy to evaluate, and is just
\begin{equation}\label{eqn:vectorBivector:441}
\begin{aligned}
b &=
\frac{
   \begin{vmatrix}
   x_1 & x_2 \\
   a_1 & a_2 \\
   \end{vmatrix}
}
{
   \begin{vmatrix}
   x_1 & x_2 \\
   y_1 & y_2 \\
   \end{vmatrix}
} \\
d &=
\frac{
   \begin{vmatrix}
   x_1 & x_2 \\
   b_1 & b_2 \\
   \end{vmatrix}
}
{
   \begin{vmatrix}
   x_1 & x_2 \\
   y_1 & y_2 \\
   \end{vmatrix}
}
\end{aligned}.
\end{equation}
To similarly solve for \( a, c \), one just needs to wedge with \( \By \) instead.

Observe that if we transpose each of these determinants above, which does not change them, then that is the standard form for a Cramer's rule solution for a two dimensional system.

\subsection{Projection and rejection.}
There is an analogous filtering property of the dot product too that is worth recalling.  This also helps bring a bit of geometry back into the picture.
\maketheorem{Dot product filtering property.}{thm:vectorBivector:9}{
Given vectors \( \Bx, \By, \Bz \), where \( \Bx \cdot \Bz = 0 \)
\begin{equation*}
\Bx \cdot \lr{ \By + a \Bz } = \Bx \cdot \By.
\end{equation*}
} % theorem
Proof is left to the reader, but only requires the linearity property of the dot product.  While the wedge product filters out components of a vector that are parallel, the dot product filters out components of a vector that are perpendicular.  Using this property, we can formulate projection and rejection operators.

\maketheorem{Projection and rejection.}{dfn:vectorBivector:17}{
Given vectors \( \Bx, \Bu \), the component of \( \Bx \) in the direction of \( \Bu \) is
\begin{equation*}
\ProjOp{\Bu}{\Bx} = \lr{ \Bx \cdot \ucap } \ucap,
\end{equation*}
where \( \ucap = \Bu/\Norm{\Bu} \), and the component of \( \Bx \) perpendicular to the direction \( \Bu \) is
\begin{equation*}
\Rej{\Bu}{\Bx} = \lr{ \Bx \wedge \ucap } \ucap = \ucap \lr{ \ucap \wedge \Bx } = \Bx - \lr{ \Bx \cdot \ucap } \ucap.
\end{equation*}
This is illustrated in \cref{fig:projection:projectionFig1}.
\imageFigure{../figures/blogit/projectionFig1}{Projection and rejection.}{fig:projection:projectionFig1}{0.3}
} % theorem
\begin{proof}
We use the standard trick of multiplying by a special form of one.  For example, we may expand \( \Bx \) by multiplying it by \( \ucap \ucap = 1 \) on the right
\begin{equation}\label{eqn:vectorBivector:501}
\begin{aligned}
\Bx
&= \Bx \ucap \ucap \\
&= \lr{ \Bx \ucap } \ucap \\
&= \lr{ \Bx \cdot \ucap + \Bx \wedge \ucap } \ucap \\
&= \lr{ \Bx \cdot \ucap } \ucap + \lr{ \Bx \wedge \ucap } \ucap,
\end{aligned}
\end{equation}
or on the left
\begin{equation}\label{eqn:vectorBivector:521}
\begin{aligned}
\Bx
&= \ucap \ucap \Bx \\
&= \ucap \lr{ \ucap \Bx } \\
&= \ucap \lr{ \ucap \cdot \Bx + \ucap \wedge \Bx } \\
&= \ucap \lr{ \ucap \cdot \Bx } + \ucap \lr{ \ucap \wedge \Bx }.
\end{aligned}
\end{equation}
In both expansions, we find that \( \Bx \) has a component \( \lr{ \Bx \cdot \ucap } \ucap \), a vector that is clearly colinear with \( \Bu \).  If we can show that the remainder \( \Bx - \lr{ \Bx \cdot \ucap } \ucap = \lr{ \Bx \wedge \ucap } \ucap = \ucap \lr{ \ucap \wedge \Bx } \) is perpendicular to that, then we are done.  Taking dot products using grade zero selection, we have
\begin{equation}\label{eqn:vectorBivector:541}
\begin{aligned}
\lr{ \lr{ \Bx \cdot \ucap } \ucap } \cdot \lr{ \ucap \lr{ \ucap \wedge \Bx } }
&=
\gpgradezero{ \lr{ \Bx \cdot \ucap } \ucap \ucap \lr{ \ucap \wedge \Bx } } \\
&=
\lr{ \Bx \cdot \ucap } \gpgradezero{ \ucap \wedge \Bx } \\
&= 0,
\end{aligned}
\end{equation}
since the grade-zero selection of a bivector is zero.
\end{proof}
\section{Vector-bivector products.}
Let's now look at the product of a bivector and a vector.  Some examples to start with will be helpful.

Let \( B = \Be_1 \Be_2 \), \( \Bx = a \Be_1 + b \Be_2 \).  The product \( B \Bx \) is
\begin{equation}\label{eqn:vectorBivector:561}
\begin{aligned}
B \Bx
&=
\Be_1 \Be_2 \lr{ a \Be_1 + b \Be_2 } \\
&=
-a \Be_2 \Be_1^2 + b \Be_1 \Be_2^2 \\
&=
-a \Be_2 + b \Be_1 \\
\end{aligned}
\end{equation}
In this case, with ``coplanar'' bivector and vector, the bivector multiplication rotates the vector.

Let \( B = \Be_1 \Be_2 \), \( \Bx = \Be_3 \).  The product \( B \Bx \) is
\begin{equation}\label{eqn:vectorBivector:581}
B \Bx
= \Be_1 \Be_2 \Be_3.
\end{equation}
In this case, when the bivector factors are all perpendicular to the vector, the multiplication yields a trivector.

It's clear that a bivector-vector product, in general, has both a vector and a trivector component, in particular
\begin{equation}\label{eqn:vectorBivector:601}
B \Bx = \gpgradeone{ B \Bx } + \gpgradethree{ B \Bx }.
\end{equation}
Using the generalized definitions of the dot and wedge product, such a decomposition can be written as
\begin{equation}\label{eqn:vectorBivector:621}
B \Bx = B \cdot \Bx + B \wedge \Bx,
\end{equation}
however, we have to figure out how to evaluate these dot and wedge products.

\maketheorem{Bivector-vector dot product}{thm:vectorBivector:77}{
Given a bivector \( \Ba \wedge \Bb \), and vector \( \Bc \),
\begin{equation*}
\lr{ \Ba \wedge \Bb } \cdot \Bc = \lr{ \Bb \cdot \Bc } \Ba - \lr{ \Ba \cdot \Bc } \Bb.
\end{equation*}
} % theorem
\begin{proof}
Given \( \Bc = \sum_k c_k \Be_k \), and
\begin{equation}\label{eqn:vectorBivector:641}
\Ba \wedge \Bb = \sum_{i < j}
\begin{vmatrix}
a_i & a_j \\
b_i & b_j \\
\end{vmatrix} \Be_i \Be_j,
\end{equation}
the bivector-vector dot product is
\begin{equation}\label{eqn:vectorBivector:661}
\lr{ \Ba \wedge \Bb } \cdot \Bc
=
\sum_{i < j, k}
c_k
\begin{vmatrix}
a_i & a_j \\
b_i & b_j \\
\end{vmatrix}
\gpgradeone{
\Be_i \Be_j \Be_k }.
\end{equation}

Of this sum, only the \( k = i \), or \( k = j \) terms will contribute to the vector grade selection.
%\begin{equation}\label{eqn:vectorBivector:681}
%\sum_{i < j, k}
%=
%\sum_{i < (k = j)} + \sum_{(k = i) < j} + \sum_{i < j < k} + \sum_{i < k < j} + \sum_{k < i < j}.
%\end{equation}
That is
%The first two terms will be the vector grade of the product, and the last three terms will contribute grade three terms.  For the vector part, we have
\begin{equation}\label{eqn:vectorBivector:701}
\begin{aligned}
\lr{ \Ba \wedge \Bb } \cdot \Bc
&=
\lr{ \sum_{i < (k = j)} + \sum_{(k = i) < j} }
c_k
\begin{vmatrix}
a_i & a_j \\
b_i & b_j \\
\end{vmatrix}
\Be_i \Be_j \Be_k \\
&=
\sum_{i < j}
c_j
\begin{vmatrix}
a_i & a_j \\
b_i & b_j \\
\end{vmatrix}
\Be_i \Be_j \Be_j
c_i
\begin{vmatrix}
a_i & a_j \\
b_i & b_j \\
\end{vmatrix}
\Be_i \Be_j \Be_i
\\
&=
\sum_{i < j}
\lr{ c_j \Be_i - c_i \Be_j}
\begin{vmatrix}
a_i & a_j \\
b_i & b_j \\
\end{vmatrix}.
\end{aligned}
\end{equation}
Our brain dead approach has resulted in an ugly mess, but we can simplify it.  We use the standard index gymnastics trick of taking half of double the quantity, to find
\begin{equation}\label{eqn:vectorBivector:721}
\begin{aligned}
\lr{ \Ba \wedge \Bb } \cdot \Bc
&=
\inv{2}
\lr{
   \sum_{i < j}
   \lr{ c_j \Be_i - c_i \Be_j }
   \begin{vmatrix}
   a_i & a_j \\
   b_i & b_j \\
   \end{vmatrix}
   +
   \sum_{j < i}
   \lr{ c_i \Be_j - c_j \Be_i }
   \begin{vmatrix}
   a_j & a_i \\
   b_j & b_i \\
   \end{vmatrix}
} \\
&=
\inv{2}
\lr{
   \sum_{i < j}
   \lr{ c_j \Be_i - c_i \Be_j }
   \begin{vmatrix}
   a_i & a_j \\
   b_i & b_j \\
   \end{vmatrix}
   +
   \sum_{j < i}
   \lr{ c_j \Be_i - c_i \Be_j }
   \begin{vmatrix}
   a_i & a_j \\
   b_i & b_j \\
   \end{vmatrix}
} \\
&=
\inv{2} \sum_{ij}
   \lr{ c_j \Be_i - c_i \Be_j }
   \begin{vmatrix}
   a_i & a_j \\
   b_i & b_j \\
   \end{vmatrix}.
\end{aligned}
\end{equation}
In the last step, we are able to add in the \( i = j \) term to sum over all pairs of \( i, j \), because the \( i = j \) factors are zero.  This can now be expanded to find
\begin{equation}\label{eqn:vectorBivector:741}
\begin{aligned}
\lr{ \Ba \wedge \Bb } \cdot \Bc
&=
\inv{2} \sum_{ij}
\lr{
c_j \Be_i a_i b_j - c_j \Be_i a_j b_i - c_i \Be_j a_i b_j + c_i \Be_j a_j b_i
} \\
&=
\inv{2}
\lr{
\lr{ \Bc \cdot \Bb } \Ba
-\lr{ \Bc \cdot \Ba } \Bb
-\lr{ \Bc \cdot \Ba } \Bb
+\lr{ \Bc \cdot \Bb } \Ba
} \\
&=
\lr{ \Bc \cdot \Bb } \Ba - \lr{ \Bc \cdot \Ba } \Bb.
\end{aligned}
\end{equation}
\end{proof}
\maketheorem{Bivector-vector wedge product.}{thm:vectorBivector:37}{
Given a bivector \( \Ba \wedge \Bb \), and vector \( \Bc \),
\begin{equation*}
\Ba \wedge \Bb \wedge \Bc =
\sum_{i < j < k}
\begin{vmatrix}
a_i & a_j & a_k \\
b_i & b_j & b_k \\
c_i & c_j & c_k \\
\end{vmatrix}
\Be_i \Be_j \Be_k.
\end{equation*}
} % theorem
Repeated wedge products are characterized by complete antisymmetry.

We will prove this for \( \lr{ \Ba \wedge \Bb } \wedge \Bc \), and leave it to the reader to show that we get the same result for \( \Ba \wedge \lr{ \Bb \wedge \Bc } \), showing that such a repeated wedge product is associative.

\begin{proof}
Using \cref{eqn:vectorBivector:641}, we may write
\begin{equation}\label{eqn:vectorBivector:761}
\lr{ \Ba \wedge \Bb } \wedge \Bc
=
\sum_{i < j, k}
c_k
\begin{vmatrix}
a_i & a_j \\
b_i & b_j \\
\end{vmatrix}
\gpgradethree{
\Be_i \Be_j \Be_k }.
\end{equation}
This time, only the \( i \ne j \ne k \) terms will contribute to the sum, and we may decompose the summation into three pieces
\begin{equation}\label{eqn:vectorBivector:781}
\begin{aligned}
\lr{ \Ba \wedge \Bb } \wedge \Bc
&=
\lr{ \sum_{i < j < k} + \sum_{i < k < j} + \sum_{k < i < j} }
c_k
\begin{vmatrix}
a_i & a_j \\
b_i & b_j \\
\end{vmatrix}
\Be_i \Be_j \Be_k \\
&=
\sum_{i < j < k}
c_k
\begin{vmatrix}
a_i & a_j \\
b_i & b_j \\
\end{vmatrix}
\Be_i \Be_j \Be_k
+
\sum_{i < j < k}
c_j
\begin{vmatrix}
a_i & a_k \\
b_i & b_k \\
\end{vmatrix}
\Be_i \Be_k \Be_j
+
% k -> i
% i -> j
% j -> k
\sum_{i < j < k}
c_i
\begin{vmatrix}
a_j & a_k \\
b_j & b_k \\
\end{vmatrix}
\Be_j \Be_k \Be_i.
\end{aligned}
\end{equation}
Here we've swapped \( k, j \) indexes in the second sum, and done a cyclic index permutation in the last.  This gives us
\begin{equation}\label{eqn:vectorBivector:801}
\begin{aligned}
\lr{ \Ba \wedge \Bb } \wedge \Bc
&=
\sum_{i < j < k}
\lr{
c_k
\begin{vmatrix}
a_i & a_j \\
b_i & b_j \\
\end{vmatrix}
-
c_j
\begin{vmatrix}
a_i & a_k \\
b_i & b_k \\
\end{vmatrix}
+
c_i
\begin{vmatrix}
a_j & a_k \\
b_j & b_k \\
\end{vmatrix}
} \Be_i \Be_j \Be_k \\
&=
\sum_{i < j < k}
\begin{vmatrix}
c_i & c_j & c_k \\
a_i & a_j & a_k \\
b_i & b_j & b_k \\
\end{vmatrix}
\Be_i \Be_j \Be_k.
\end{aligned}
\end{equation}
We may perform a cyclic row permutation of the determinant to finish the job.
\end{proof}

%}
%\EndArticle
\EndNoBibArticle
