%}}
% Copyright © 2020 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%{
\input{../latex/blogpost.tex}
\renewcommand{\basename}{multivector}
\renewcommand{\dirname}{notes/ece1228-electromagnetic-theory/}

\input{../latex/peeter_prologue_print2.tex}

\usepackage{peeters_braket}
\usepackage{peeters_figures}
\usepackage{siunitx}
\usepackage{verbatim}
\usepackage{peeters_tablebox}
\usepackage{peeters_layout_exercise}
\usepackage{macros_qed}
\usepackage{xcolor}

% "#00aa00"
\definecolor{GreenDarker}{rgb}{0, 0.67, 0}
% "#0000aa"
\definecolor{BlueDarker}{rgb}{0, 0, 0.67}
% "#550055"
\definecolor{PurpleDarker}{rgb}{0.333333, 0, 0.333333}
% "#aa0000"
\definecolor{RedDarker}{rgb}{0.67, 0, 0}

\newcommand{\DarkerGreen}[1]{{\color{GreenDarker}#1}}
\newcommand{\DarkerBlue}[1]{{\color{BlueDarker}#1}}
\newcommand{\DarkerPurple}[1]{{\color{PurpleDarker}#1}}
\newcommand{\DarkerRed}[1]{{\color{RedDarker}#1}}

\newcommand{\nbcite}[2]{%
#2%
%\itemCite{GAelectrodynamics}{#1}{#2}%
}

% \mathImageFigure{path}{caption}{label}{width}{nbpath}
% nbpath like: ps2b:countItersAndPlot.m
\newcommand{\mathImageFigure}[5]{%
\imageFigure{#1}{\nbcite{#5}{#2}}{#3}{#4}
}

\beginArtNoToc

\generatetitle{An axiomatic introduction of multivectors, vector products, and geometric algebra.}
%\chapter{Multivector}
%\label{chap:multivector}

%\section{What's in the pipe.}
%\input{inthepipe.tex}
%\section{Vectors as arrows.}
%\input{vectorasarrow.tex}
\section{Coordinate vectors.}
\subsection{Motivation.}
Having covered the basic operations of vectors in their arrow representation, we are ready to go algebraic on the subject.
In particular, while
it is easy to compute the length of a vector that has an arrow representation, where
one simply lines a ruler of appropriate units along the vector and measures, this graphical procedure is cumbersome when we need to calculate.

Some mathematical baggage from linear algebra is required to do this for general N-dimensional spaces, including
\begin{itemize}
\item coordinates,
\item basis (plural bases),
\item linear dependence and independence,
\item span,
\item dot product, and
\item metric.
\end{itemize}
None of these are hard concepts, but they interfere with the flow of the story, so let's cheat.  We can temporarily avoid the ideas of linear dependence, independence and span, by restricting the story to 2D and 3D spaces that we can describe geometrically.
%When we eventually finish constructing our geometric algebra toolbox, we will have a number of coordinate free methods available to us.
%but we need to understand coordinates to get to that point.
%We also need to understand coordinates, both to read the literature, and in practice.
%Coordinates and non-orthonormal bases are also a good way to introduce non-Euclidean metrics.
\subsection{Basis.}
\makedefinition{Basis (cheat).}{dfn:multivector:180}{
Given a periodic partitioning of a 2D(3D) space into repeated parallelograms(parallelopipeds), an ordered pair(triplet) of vectors that lie between the vertices of one of the cells is called a basis.
} % definition
\index{basis}
The plural of basis is bases.
\index{bases}
A basis for a space subdivided into a parallopiped grid is illustrated with points at all the lattice vertices in
\cref{fig:fbasis:fbasisFig1}.
\imageFigure{../figures/GAelectrodynamics/fbasisFig1}{A basis for a 2D space with a parallopiped lattice.}{fig:fbasis:fbasisFig1}{0.3}
The basis in this example is the ordered set \( \setlr{\Bf_1, \Bf_2} \).
There is no unique choice of basis for a given lattice, as different orderings and directions are possible (
\( \setlr{\pm \Bf_2, \pm \Bf_1} \) or \( \setlr{\pm \Bf_1, \pm \Bf_2} \)).
Alternate bases for this lattice include \( \setlr{\Bf_2, \Bf_1} \), and \( \setlr{-\Bf_1, -\Bf_2} \).
A basis has one and only one vector that lies between the vertices of the smallest cell of the lattice.
%\imageTwoFigures
%{../figures/GAelectrodynamics/fbasisFig1}
%{../figures/GAelectrodynamics/ebasisFig1}
%{Oblique and rectangular two dimensional bases.}{fig:ebasis:ebasisFig1}{scale=0.4}

Of course, the simplest possible lattice is that of a uniform square grid as illustrated in
\cref{fig:ebasis:ebasisFig1}.
\imageFigure{../figures/GAelectrodynamics/ebasisFig1}{A basis for a 2D space with a square lattice.}{fig:ebasis:ebasisFig1}{0.3}
In this example, the pair of vectors \( \setlr{\Be_1, \Be_2}\) is our basis, as they lie along the respective lattice directions.
Alternate bases for this square grid include \( \setlr{\Be_2, \Be_1} \), \( \setlr{-\Be_1, -\Be_2} \), and \( \setlr{\Be_2, -\Be_1} \).
\subsection{Coordinates.}
\makedefinition{Coordinates.}{dfn:multivector:200}{
Given a basis with \( N \) basis vectors \( \setlr{ \Bf_1, \cdots \Bf_N } \), and a vector \( \Bx = \sum_{i = 1}^N a_i \Bf_i \), the coordinates of the vector \( \Bx \) are \((a_1, a_2, \cdots a_N)\), or in matrix (column vector) notation
\begin{equation*}
\begin{bmatrix}
a_1 \\
a_2 \\
\vdots \\
a_N
\end{bmatrix}.
\end{equation*}
} % definition
We are restricting attention for now to \( N \le 3 \), since it's not terribly fun to try to visualize a repeating hyper-parallopipeds lattice (perhaps some people would describe that visualization game as fun, but even so, it is probably headache inducing.)

We may use the two previous lattice examples to illustrate the idea of coordinates.  For example, consider
\( \Bx = 5 \Bf_1 + 3 \Bf_2 \) as illustrated in
\cref{fig:fbasisSum:fbasisSumFig1}.
\imageFigure{../figures/GAelectrodynamics/fbasisSumFig1}{Decomposition of a particular vector in terms of an oblique set of basis vectors.}{fig:fbasisSum:fbasisSumFig1}{0.3}
The coordinates of this vector with respect the basis \(\setlr{\Bf_1, \Bf_2}\) are \((5,3)\).  Any set of coordinates must be implicitly associated with an underlying basis, and are meaningless without such an association.  For example, the
coordinates of \(\Bx\) with respect the basis \(\setlr{-\Bf_2, \Bf_1}\) would be \((-3,5)\).
As another example, consider the vector \( \By = 3 \Be_1 + 2 \Be_2 \) as illustrated in
\cref{fig:ebasisSum:ebasisSumFig1} for a square lattice.
\imageFigure{../figures/GAelectrodynamics/ebasisSumFig1}{Decomposition of a particular vector on a square lattice.}{fig:ebasisSum:ebasisSumFig1}{0.3}
The coordinates of \( \By \) with respect to the basis \( \setlr{\Be_1, \Be_2}\) are \((3,2)\).
The coordinates of \( \By \) with respect to the an alternate basis \( \setlr{\Be_2, -\Be_1}\) would be \((2,-3)\), that is
\(\By = 2 \Be_2 + (-3)(-\Be_1)\).

The special case of a cubic lattice is so important that we give it a name
\index{standard basis}
\makedefinition{Standard basis.}{dfn:multivector:220}{
When the lattice for a basis is unit square(volume), we call such a basis ``the standard basis'' and designate it with a basis \( \setlr{\Be_1, \Be_2, \cdots, \Be_N}\).
} % definition
You may ask why we could possibly care to use anything but a square(cubic) lattice.  Here are a few reasons
\begin{itemize}
\item When we get to vector calculus, we require bases that vary from point to point, depending on the parametization of the space that we are using in our integrals.  Those parameterizations (spherical, cylindrical, ...) need not be cubic.
\item There are important applications for such bases in solid state physics, as the crystal structures that occur due to molecular bonds are not friendly enough to restrict themselves to cubic configurations.
\item Developing the toolbox for more general bases and coordinates will leave us ready to tackle the non-Euclidean space (spacetime, or ``four-vectors'') that we encounter in special relativity and electromagnetism.  All electromagnetic theory is relativistic, so having the tools to express these ideas is not just academic.
\end{itemize}

\section{OLD.}
\subsection{Coordinates.}
\subsection{Vector length operation.}
\subsection{Vector dot product.}
In engineering, vectors are usually represented as \( N \times 1 \) column matrixes\footnote{Matrices are like meth or crack to engineers, who will attempt to cast everything as a matrix problem.}.
A vector \( \Bx \) with coordinates \((x,y,z)\) would have the representation
\begin{dmath}\label{eqn:multivector:60}
\Bx =
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}.
\end{dmath}
With a coordinate representation, we can add and subtract vectors by simply adding and subtracting their coordinates.
For example, with
\(
\Bx =
{
\begin{bmatrix}
x_1 &
x_2 &
x_3
\end{bmatrix}
}^\T,
\By =
{\begin{bmatrix}
y_1 &
y_2 &
y_3
\end{bmatrix}}^\T \), addition and scaling operations are trivial
\begin{equation}\label{eqn:multivector:140}
a \Bx + b \By
=
a
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
+
b
\begin{bmatrix}
y_1 \\
y_2 \\
y_3
\end{bmatrix}
=
\begin{bmatrix}
a x_1 + b y_1 \\
a x_2 + b y_2 \\
a x_3 + b y_3
\end{bmatrix}.
\end{equation}

We may decompose a vector into components in each of the ``unit directions'' as follows
\begin{dmath}\label{eqn:multivector:160}
\Bx =
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
+
x
\begin{bmatrix}
1 \\
0 \\
0 \\
\end{bmatrix}
+
y
\begin{bmatrix}
0 \\
1 \\
0 \\
\end{bmatrix}
+
z
\begin{bmatrix}
0 \\
0 \\
1 \\
\end{bmatrix}.
\end{dmath}
The vectors with a single one value in each of the directions may be interpretted as unit vectors, vectors with length one in each of the respective directions.
If we write
\begin{equation}\label{eqn:prerequisites:20}
\Be_1 =
\begin{bmatrix}
1 \\
0 \\
0 \\
\end{bmatrix},\quad
\Be_2 =
\begin{bmatrix}
0 \\
1 \\
0 \\
\end{bmatrix},\quad
\Be_3 =
\begin{bmatrix}
0 \\
0 \\
1 \\
\end{bmatrix}.
\end{equation}
then the matrix representation of the vector takes the form
\begin{dmath}\label{eqn:prerequisites:40}
\Bx = x \Be_1 + y \Be_2 + z \Be_3.
\end{dmath}
Here we have assumed that the set of vectors
\( \setlr{ \Be_1, \Be_2, \Be_3 } \) were all mutually perpendicular.
With that assumption, we call such an ordered set the standard basis.
This is a loose definition, since a proper definition of basis depends on the concept of linear independence\footnote{It is assumed that basis, linear dependence and linear independence are all familiar concepts, but I won't take the time to define them systematically yet.}.
%independent of whether the underlying representation of the unit vectors themselves are tuple, row, column, or anything else.
%
%The simplest interpretation of each of these
%There is a built in ambguity in a coordinate vector representation.
%, what can be described as the problem of implied basis.
%For example a vector \( \Bx \) with coordinates \( x, y, z \) is
%
%\subsection{Unit vectors: include?}
%In particular, the unit vectors in the x, y, z directions are
%I'll use the symbol \( \Be_i \) to designate the unit vector in the ith direction%
%\footnote{
%The notation for the unit vectors themselves varies by author.
%For example, it's not uncommon in engineering texts to use \( \hat{a}_x, \hat{a}_y, \hat{a}_z \) instead of \( \Be_1, \Be_2, \Be_3 \).  I would guess that the \(\hat{a}\) notation evolved to avoid the overloading of the symbol \(e = 2.718\cdots\).}.
%This symbolic designation allows any vector to be encoded in a
%representation agnostic fashion.
%

\subsection{Metric and dot product.}
Our niave description of a vector is a quantity with direction and magnitude.
The
coordinate representation of a vector encodes the direction, at least assuming that a vector is anchored at the zero vector (the origin), and the arrow head sits at the location specified by the coordinates.
However, we must also supplement the coordinate representation with an operation that provides the length of the vector.
That operation is the dot product, which has the following general form
\begin{dmath}\label{eqn:multivector:101}
\Bx \cdot \By = \Bx^\T G \By,
\end{dmath}
where \( G \) is a symmetric matrix, called the metric.
When \( G = I \), the identity matrix, then the dot product provides a compact encoding of the Euclidean length
\begin{equation}\label{eqn:multivector:80}
\Norm{\Bx} = \sqrt{ \Bx \cdot \Bx } = \sqrt{x^2 + y^2 + z^2}.
\end{equation}
The Euclidean dot product (i.e. \( G = I \)) has an absolute maximum value when the two vectors are colinear, and is zero when the two vectors are perpendicular.

The idea of a metric may appear to be useless abstraction, but it has applications in both physics and computer graphics.  In particular, the metric for the four-vectors of special relativity is
\begin{dmath}\label{eqn:multivector:120}
G =
\pm
\begin{bmatrix}
-1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}.
\end{dmath}
Vectors can be characterized as either
``timelike'' or ``spacelike'', depending on the sign of their
``squared-length''.
This special relativistic dot product does not have the usual \( \Bx \cdot \Bx \ge 0 \) property that is usually included in an axiomatic definition of a dot product.

An engineering student may ask, ``Why would I care about special relativity?''.
Unlike some fields of physics (like classical and quantum mechanics), electromagnetism is relativistically correct to start with, requiring no corrections to generalize it.  This means that the relativisitic toolbox can be used to solve some problems with less effort.  This is one of the reasons why we care to setup our algebraic toolbox in a way that will work for both relativistic problems and the everyday 3D Euclidean geometry problems that surround us.

Non-identity metrics are also of interest in some computer graphics applications.  The interested reader is referred to the
conformal and projective geometric algebra literature for details, as the are beyond the scope of this work\footnote{The basic idea is that extra dimensions are introduced to represent the origin and to represent a point-at-infinity.}.
If all this seems too complicated, one of the saving graces is that the metric that is used in all the
geometric algebra applications that I'm aware of is always a
diagonal matrix with diagonal values that have values \( 0, \pm 1 \).

%\section{Vector space.}
%\input{vectorspace.tex}
%}
\EndArticle
